{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7_hx8SJJR4-"
      },
      "source": [
        "# Assignment 2 - Logistic Regression & Naive Bayes\n",
        "\n",
        "Due by 11:59pm, Oct 16, 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyGrk2X_J_1h"
      },
      "source": [
        "## Theory Questions (Full points: 40, each question 5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8OnMEI0JXxA"
      },
      "source": [
        "1. Explain the importance of setting up learning rate in the gradient descent based methods. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv3iz5dBJX7Z"
      },
      "source": [
        "**Answer:**\n",
        "First of all, the ***learning rate*** is a parameter with positive value between ***0.0 and 1.0*** that can be modified in order to establish the speed at which the model adapts to the problem. That is to say, it works as a multiplyer of the gradient result in order to move from one coordinate to another during the gradient descent. The larger is its value, the faster the coordinates move in order to find the global minimum of the cost function, so the faster the wiegths are changed. In case of using large learning rates, less epochs are required. \n",
        "\n",
        "Therefore, adjusting the learning rate of the model is a vital point when designing the model due to the fact that a large one could end up in a convergence to a non optimal point; whereas a small one could wind up in getting the model stuck. That is why the learning rate is the most important hyperparameter and should be tuned in the first place ahead of other ones that have less importance even though they might also be taken into account.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO8kErgCJX-P"
      },
      "source": [
        "2. What is the stochastic gradient descent? Why do we need stochastic gradient descent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEviYAL2JeWs"
      },
      "source": [
        "**Answer:**\n",
        "The Stochastic Gradient Descent (SGD) is a improved method for dradient descent when the order of the data is to large. In order to avoid our algorithm to perform too many calculations per iterations, the SDG holds really good advantages in regards of computational costs.\n",
        "\n",
        "The SDG technique is based on the fact of only choosing one random data point in order to perform the gradient calculations instead of taking the whole dataset while calculating the gradient descent (GD) as it is done in the normal gradient descent technique. In order to take advantage of both, the accuracy of the GD and the speed of the SDG, it can also be used the mini-batch technique that stands for the idea of using a sample of random data points instead of one or the whole dataset. This way, the benefits of both methods are combined. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vQD3sgwJYBS"
      },
      "source": [
        "3. Explain the reasons to perform feature scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-e37RdDJfmd"
      },
      "source": [
        "**Answer:**\n",
        "The feature scaling usually appears in the data pre-processing and it deals with the fact of trying to establish some standard rules in the independent features of the data set. \n",
        "\n",
        "For instance, in a classification model trained with a dataset, we want to predict based on two atributtes, wheter if teh result will be YES or NO, a binary result. Therefore, we must plot a two dimensional scatter graph with the data set and draw the points with the known results as YES or NO. We will see that probably the YES data points will be backed up in a certan space of the graph, and the NO data points will be focused on another part. Therefore we can predict that every new point close to each ceratin space will be predicted as YES or NO, but what about the data points that are between the two certain zones? \n",
        "\n",
        "In order to predict then, we should calculate the distance from data point to the centroids of the YES and NO sample points, and predict that data point as YES or NO depending on which centroid is closer. But that distance can be calculated using several methods such as the ***Euclidean*** distance, ***Manhattan*** distance and ***Minkowski*** distance.\n",
        "\n",
        "In case of the Manhattan distance, each coordinate or attribute of the centroid is substracted by the one of the data point and that absolute positive results will be added all over the coordinates. As it is the sum of differences, if the order of one of the attributes is higher than the others, for example, salaries (100k) vs the number of houses owned (2), the salaries will always be dominant because a difference of 1k in salaries will mean a 1k difference of owned houses, which will be almost impossible. Therefore, in order to avoid this kind of error that lead to dominant feature and wrong predictions, the features must be scaled relatively in a fixed range as (-1,1) or (0,1) in order to standarize the data and make sense to the sum oof the differences. This way, the added items will be of the same order of magnitude.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOqhvEpuJYEX"
      },
      "source": [
        "4. What is the probabilistic generative model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf5JjDFDJe6G"
      },
      "source": [
        "**Answer:**\n",
        "The probabilistic generative model is often used in order to create new datasets of a given dataset. That is to say, the generative model will use a training dataset in order to recognize the pattern of the type of dataset, and then, it will be able to create new datasets of the same kind of data than the one it has learned from. \n",
        "\n",
        "Therefore, if we give to our generative model a dataset in form of car images and it use them in order to train itself. After having finished the training, it will be able to create new images of cars as it has learned which are the rules that governs the creation of car images and which features chacarterize better to a car. \n",
        "\n",
        "In addition, a generative model must be probabilistic rather than deterministic. That is to say, for same inputs, the model should give different outputs because it must be based on same randomness or stochastic distribution that influences the results of the dataset. For example, a generative model that merely calculates an average may not be generative, because for the same input numbers, it will always obtain the same output average.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8V802XDJjxe"
      },
      "source": [
        "5. Explain how we perform maximum likelihood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3PrgB7kJl99"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "***Maximum Likelihood Estimation (MLE)*** has a lot to do with the fact of trying to fit a distribution to the data set that we have. This is useful for making easier the data treatment by replacing the points cloud by a statistical distribution. \n",
        "\n",
        "One of the most used distributions is the normal one, where most of the measurements are close to the mean, and low and behold, the measurements are simetrical (not perfectly, but at least there are not skewed to one side). \n",
        "\n",
        "So, in order to replace the data by a normal distribution, we have to plot the likelihood of a normal distribution centered in one point of the dataset, to accurately represent our data set. This centered normal distribution will be swifted forward in order to get the spectrum of likelihood of being correctly centered. Once finished, our model should look for the normal distribution which is centered in a point that gives the Maximum Likelihood, and will choose that normal distribution because it defines the best way our cloud of data points. The MLE will be done for both, the mean and the standard deviation. \n",
        "\n",
        "In summary, the MLE deals with the process of getting the spectrum of the likelihood of properly replacing the cloud of data points by a normal distribution, and then taking the maximum likelihood point. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kUcM98LJj9N"
      },
      "source": [
        "6. Explain the reasons about using cross entropy loss in logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "swtMMLse9Mlc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuECRF2PJk81"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The cross entropy loss function is one of the loss functions that can be used in ***Machine Learning *** and *** Optimization ***problems.\n",
        "\n",
        "In fact, the cross entropy function is usually used in logistic regressions in order to take advantage of the following scenarios.\n",
        "\n",
        "The maximun of the log-likelihood function will be the same as the regular likelihood function. The benefit o using the log-likelihood is that it can avoid the numerical underflow when the probabilities are too low. Therefore, we will proceed to minimize the negative of the log-likelihood function (cross-entropy error function), which is the same as maximizing the log-likelihood function. \n",
        "\n",
        "Another reason for choosing the cross-entropy function is the fact that in simple regression the output result in a convex loss function, in which the global minimum will be easy to find.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8bCRVRaJkBP"
      },
      "source": [
        "7. Explain the differences between discriminative and generative model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2KIVplzJlYt"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The probabilistic generative model is a kind of model that contrasts with the discriminative models. While the generative models can generate new data instances, for instance, can generate new photos of a cat starting from some cat photos; the discriminative one can discriminate between different kinds of data instances, for instance, it can make a distinction between cat and dog photos. \n",
        "\n",
        "Therefore, the generative model gets the joint likelihood of p(x, y), being X a set of data instances and Y a set of labels. So, it can perform predictive actions such as which word fits better in the final part of the sentence. \n",
        "\n",
        "The discriminative model works with the distribution p(y|x), which is the natural distribution for classifying a given example x into a class y. \n",
        "Unlike the generative model, the discriminative one gives the probability of a word to pertain to a sentence, it does not make up the potentially most likely word at the last part of the sentence. \n",
        "\n",
        "Examples of Generative Classifiers:\n",
        "‌Naïve Bayes, Bayesian networks, Markov random fields‌ ‌a‌n‌d‌ ‌Hidden Markov Models (HMM).\n",
        "\n",
        "Examples of Discriminative Classifiers:‌ ‌Logistic regression, Scalar Vector Machine, Traditional neural networks, Nearest neighbour and Conditional Random Fields (CRF)s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCL0MZN6JkET"
      },
      "source": [
        "8. Bishop's Book \"Pattern Recognition and Machine learning\" - Exercise 4.9\n",
        "\n",
        "Consider a generative classification model for K classes defined by\n",
        "prior class probabilities p(Ck) = πk and general class-conditional densities p(φ|Ck)\n",
        "where φ is the input feature vector. Suppose we are given a training data set {φn, tn}\n",
        "where n = 1,...,N, and tn is a binary target vector of length K that uses the 1-of\u0002K coding scheme, so that it has components tnj = Ijk if pattern n is from class Ck.\n",
        "Assuming that the data points are drawn independently from this model, show that\n",
        "the maximum-likelihood solution for the prior probabilities is given by\n",
        "πk = Nk\n",
        "N "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-h519EbJmbz"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "\n",
        "The theoretical demonstration will start by using the likelihood function as the one as follows:\n",
        "\n",
        "$\\prod\\limits_{n=1}^{N}\\prod\\limits_{k=1}^{K}(p(\\phi_n|C_k)\\pi_k)^{t_{nk}}$\n",
        "\n",
        "If the events are independent and identically distributed, the log-likelihood function will be as follows:\n",
        "$\\sum\\limits_{n=1}^N\\sum\\limits_{k=1}^Kt_{nk}·(\\ln p(\\phi_n|C_k)+\\ln \\pi_k)$\n",
        "\n",
        "The probability of $C_k$ is $p(C_k)=\\pi_k$, and by adding all the $p(C_k)$ of the different classes, we get $\\pi_k$ is $\\sum\\limits_{k=1}^K\\pi_k=1$\n",
        "\n",
        "The Lagrange equation is ${{\\mathcal {L}}(x,\\lambda )=f(x)-\\lambda \\sum\\limits_{k=1}^K\\pi_k-1}$\n",
        "\n",
        "The lilekyhood is:\n",
        "$\\sum\\limits_{n=1}^N\\sum\\limits_{k=1}^Kt_{nk}·(\\ln p(\\phi_n|C_k)+\\ln \\pi_k)-\\lambda\\big(\\sum\\limits_{k=1}^K\\pi_k-1\\big)$\n",
        "\n",
        "\n",
        "We want to maximize the log-likelihood with respect to the vector π, given the condition that the class probabilities added up together give one. \n",
        "\n",
        "$\\sum\\limits_{k=1}^K\\pi_k=1$\n",
        "\n",
        "\n",
        "If we take the differentials $\\frac{\\partial L}{\\partial \\pi_{k}}$ we have:\n",
        "$\\frac{\\partial L}{\\partial \\pi_{k}}=\\sum\\limits_{n=1}^{N} \\frac{t_{n k}}{\\pi_{k}}-\\lambda$\n",
        "$\\pi_k = \\frac{N_k}{\\lambda}$\n",
        "\n",
        "\n",
        "Taking the Lagrange function and  derivating with respect $\\lambda$ is \n",
        "\n",
        "Finally: $\\lambda$ being a Lagrange multiplier\n",
        "\n",
        "- $\\sum\\limits_{k=1}^K\\pi_k=\\sum\\limits_{k=1}^K\\frac{N_k}{\\lambda}=\\frac{N}{\\lambda}=1$\n",
        "- $\\lambda=N$\n",
        "- $\\pi_k=\\frac{N_k}{\\lambda}=\\frac{N_K}{N}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AiWTVDf7cZ2"
      },
      "source": [
        "## Programming Questions (Full points: 60, each question 30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUXdV3L376cA"
      },
      "source": [
        "# Do not edit the codes in this cell\n",
        "# load required library\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import random \n",
        "import math\n",
        "\n",
        "\n",
        "# load dataset\n",
        "x, y = load_iris(return_X_y=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iwaee_T-Urj"
      },
      "source": [
        "### - Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M1DAu9XPY8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca9e591d-ff4f-4876-fb11-62bcf213fc00"
      },
      "source": [
        "# Do not edit the codes in this cell\n",
        "# We use 100 samples and 2 features for logistic regression function\n",
        "#X = X[:-100]\n",
        "#y = y[:-100]\n",
        "\n",
        "x_lr = x[:100, :2] # class 0 and 1 balanced\n",
        "y_lr = y[:100]\n",
        "\n",
        "#Visualizing the data set\n",
        "\n",
        "print(x_lr[:5])\n",
        "print(y_lr)\n",
        "\n",
        "#Printing some samples in order to see the shape of the data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.1 3.5]\n",
            " [4.9 3. ]\n",
            " [4.7 3.2]\n",
            " [4.6 3.1]\n",
            " [5.  3.6]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LdQ5yubLJld"
      },
      "source": [
        "In the assignment 2, you have more freedom on your programming design. In this part, you are going to implement your own Logistic Regression function. You need to implement logistic regression with stochastic gradient descent from scratch. The required functions are listed below. You can add more functions as you need. **No library versions of logistic regression are allowed**. \n",
        "_________\n",
        "\n",
        "1. train_test_split\n",
        "\n",
        "**Randomly** split data into train and test set. 80% of the raw data will be the train set and 20% of the raw data will be the test set.\n",
        "\n",
        "2. sigmoid\n",
        "\n",
        "The core of logistic regression\n",
        "\n",
        "3. predict\n",
        "\n",
        "Predict an output value for a given x with a set of coefficients. \n",
        "\n",
        "4. accurate\n",
        "\n",
        "Calculate accuracy percentage of the predictions.\n",
        "\n",
        "5. coef_sgd\n",
        "\n",
        "Estimate logistic regression coefficients using **stochastic gradient descent**. Using **the cross entropy loss**. Carefully choose learning rate and epochs values.\n",
        "\n",
        "6. draw_model\n",
        "\n",
        "Plot the final logistic regression model using the final coefficients and raw datasets. Plot title and xlabel, ylabel are required. Show the plot in the Output.\n",
        "\n",
        "Print out your **testing accurate**. Is it good? If not, analyze the reason in short. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeMvHu1L9Vmx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a2f68303-7944-4dd2-b073-281473dc0210"
      },
      "source": [
        "\n",
        "#1- Randomly split data into train and test set. 80% of the raw data will be the train set and 20% of the raw data will be the test set.\n",
        "x_train, x_test, y_train, y_test  =  train_test_split(x_lr, y_lr, test_size = 0.2, random_state = 0)\n",
        "\n",
        "x_train = np.c_[np.ones((len(x_train),1)),x_train]\n",
        "x_test = np.c_[np.ones((len(x_test),1)),x_test]\n",
        "\n",
        "predictions = []\n",
        "error_train = []\n",
        "error_test = []\n",
        "y_pred_train = []\n",
        "y_pred_test = []\n",
        "y_train_results = []\n",
        "\n",
        "theta = np.random.randn(3,1)\n",
        "\n",
        "\n",
        "# LAS FUNCIONES YA ESTAN DEFINIDAS EN LAS LIBRERIAS QUE IMPORTAMOS, NO HACERLAS\n",
        "#2- The core of logistic regression\n",
        "\n",
        "def sigmoid(h):\n",
        "    sig = 1 / (1 +  math.exp(-h))\n",
        "    return sig\n",
        "\n",
        "\n",
        "\n",
        "#4- Calculate accuracy percentage of the predictions.\n",
        "def accurate(y_pred, y):\n",
        "    acc = 0\n",
        "    n_sample = len(y)\n",
        "    threshold = 0.1\n",
        "    for i in range (n_sample):\n",
        "      if ((y_pred[i] - y[i]) < threshold):\n",
        "        acc += 1\n",
        "    perc = acc / n_sample * 100  \n",
        "    return round(perc, 2)      \n",
        "\n",
        "#Needed auxiliar functions\n",
        "\n",
        "def rounding(x):\n",
        "  if (x >= 1):\n",
        "    x = 1.0\n",
        "  elif (x <= 0):\n",
        "    x = 0.0\n",
        "  else:\n",
        "    x = round(x)\n",
        "  return x    \n",
        "\n",
        "#Plot the final logistic regression model using the final coefficients and raw datasets. Plot title and xlabel, ylabel are required. Show the plot in the Output\n",
        "def draw_model(y):\n",
        "  plt.xlabel(\"Iteration number\")\n",
        "  plt.ylabel(\"Error\")\n",
        "  plt.title(\"Sampling experiment\")\n",
        "  x = np.arange(0, len(y))\n",
        "  plt.scatter(x, y, color = 'red')\n",
        "  plt.show()\n",
        "\n",
        "def plotting(y, y_pred):\n",
        "  if (len(y) == len(y_pred)):\n",
        "    x = np.arange(len(y))\n",
        "    plt.xlabel(\"Samples\")\n",
        "    plt.ylabel(\"Class predictions\")\n",
        "    plt.title(\"Sampling experiment\")\n",
        "    plt.scatter(x, y, color = 'red')\n",
        "    plt.scatter(x, y_pred, color = 'blue')\n",
        "    plt.show()\n",
        "  else:\n",
        "    print(\"The graph cannot be plotted due to dimensionality problems\")\n",
        "\n",
        "#3- Predict an output value for a given x with a set of coefficients.\n",
        "#5- Estimate logistic regression coefficients using stochastic gradient descent.\n",
        "\n",
        "def sdg(x,y,learning_rate,epochs):\n",
        "    global theta\n",
        "    global error_train\n",
        "    global y_pred_train\n",
        "    iter = len(x_train)\n",
        "    \n",
        "    for i in range(epochs):\n",
        "      j = np.random.randint(0, iter-1)\n",
        "      prediction = rounding(sigmoid(x[j].dot(theta)))\n",
        "      y_pred_train.append(prediction)\n",
        "      predict = x_train[j].dot(theta)\n",
        "      prediction = rounding(sigmoid(predict))\n",
        "      #error = loss(y[j], predict)\n",
        "      error = prediction - y[j] \n",
        "      error_train.append(error)\n",
        "      y_train_results.append(y[j])\n",
        "      theta[0] = theta[0] - learning_rate*(x[j][0]* error)\n",
        "      theta[1] = theta[1] - learning_rate*(x[j][1]* error)\n",
        "      theta[2] = theta[2] - learning_rate*(x[j][2]* error)\n",
        "    draw_model(error_train)\n",
        "    plotting(y_train_results, y_pred_train)\n",
        "\n",
        "def test(x_test, y_test, theta): \n",
        "  global y_pred_test\n",
        "  global error_test \n",
        "  for j in range(len(y_test)):\n",
        "    predict = x_test[j].dot(theta)\n",
        "    prediction = rounding(sigmoid(predict))\n",
        "    y_pred_test.append(prediction)\n",
        "    error = prediction - y_test[j]\n",
        "    #error = loss(y_test[j], predict)\n",
        "    error_test.append(error)\n",
        "    \"\"\"print(\"Iteration number: \", j)\n",
        "    print(\"\\n\")\n",
        "    print(\"Error: \", error)\n",
        "    print(\"Prediction: \", prediction)\n",
        "    print(\"\\n\")\"\"\"\n",
        "  draw_model(error_test)\n",
        "  plotting(y_test, y_pred_test)\n",
        "  \n",
        "            \n",
        "#5- Estimate logistic regression coefficients using stochastic gradient descent. Using the cross entropy loss. Carefully choose learning rate and epochs values.\n",
        "def loss(target, pred):\n",
        "  print(target)\n",
        "  print(pred)\n",
        "  if (pred < 0):\n",
        "    pred = 0.001\n",
        "  elif (pred > 1):\n",
        "    pred = 0.98  \n",
        "  value = -(target * math.log(pred, 2) + (1-target) * math.log((1-pred),2)) \n",
        "  return round(value,2)\n",
        "\n",
        "learning_rate = 0.8\n",
        "epochs = 60000\n",
        "# learning rate = 0.8 because it makes the needed epochs in order to stop having errors the minimum, 37.000 epochs more or less.\n",
        "# epochs = 60K becuase it is slightly higher than the epoch oberserved in order to stop having errors. \n",
        "# I have seen that the greater learning rate, the gradient evolves faster and the needed epochs usually is lower. \n",
        "# BE CAREFUL!!! Large number of learning rates may foster instabilities and divergent effects  \n",
        "\n",
        "print(\"TRAINING GRAPHS\")\n",
        "print(\"\\n\")\n",
        "\n",
        "sdg(x_train, y_train, learning_rate, epochs)\n",
        "\n",
        "print(\"TESTING GRAPHS\")\n",
        "print(\"\\n\")\n",
        "\n",
        "test(x_test, y_test, theta)\n",
        "\n",
        "\n",
        "print(\"The accuracy in the training has been: \", accurate(y_pred_train, y_train_results))\n",
        "print(\"The accuracy in the testing has been: \", accurate(y_pred_test, y_test))\n",
        "\n",
        "#creo que no funciona porque la loss function que uso yo aqui es la del blog, que se supone que es de logistic pero es igual que la de gradiente normal\n",
        "\n",
        "#predict(x_test[0], 1)  #returns a value between 0 and 1 because is a logistic regression\n",
        "#the loss function is also different\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING GRAPHS\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TESTING GRAPHS\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy in the training has been:  98.88\n",
            "The accuracy in the testing has been:  100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eNsYoFL7pOG"
      },
      "source": [
        "### - Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTuXPz9V8jIA"
      },
      "source": [
        "In this part, you are going to implement your own naive bayes classifier. \n",
        "You need to implement naive bayes classifier from scratch **using the whole imported iris datasets x and y**. The required functions are listed below. You can add more functions as you need. **No library versions of naive bayes classifier are allowed**. \n",
        "_________\n",
        "\n",
        "1. cross_validation_split\n",
        "\n",
        "**Randomly** split data into 5 folds. That means 30 records will be in each fold.\n",
        "\n",
        "2. predict\n",
        "\n",
        "Predict the class label for a given x. \n",
        "\n",
        "3. accurate\n",
        "\n",
        "Calculate accuracy percentage of the predictions. Remember to average the results of k folds\n",
        "\n",
        "4. gaussian_probability\n",
        "\n",
        "Calculate the Gaussian probability distribution function for the given x. \n",
        "\n",
        "![Image_20210920015911.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "5. class_probability\n",
        "\n",
        "Calculate the probabilities of predicting each class for a given x using naive bayes algorithm. Be aware that we have multiple input variables. And you may want to use gaussian_probability function here.\n",
        "\n",
        "*Print out your accuracy result. Is it good? If not, analyze the reason in short. *\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-jO9XrADS92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5230b9f7-7d9e-4629-e56d-b5fd18009025"
      },
      "source": [
        "x = []\n",
        "for i in range(len(x_lr)):\n",
        "  array = x_lr[i] \n",
        "  x.append([array[0], array[1], y_lr[i]])\n",
        "\n",
        "\n",
        "class data():\n",
        "  pass\n",
        "\n",
        "\n",
        "def stats(x, index):  \n",
        "  length = len(x)\n",
        "  sum_avg = 0\n",
        "  std = 0\n",
        "  for i in range(length):\n",
        "    sum_avg += x[i][index]\n",
        "  avg = round(sum_avg/length, 2)\n",
        "  for i in range(length):\n",
        "    std += math.pow(x[i][index] - avg, 2) \n",
        "  std = math.sqrt(std / length)\n",
        "  \"\"\"\n",
        "  print(\"Average: \", avg)\n",
        "  print(\"Standard deviation: \", std)\n",
        "  print(\"\\n\")\n",
        "  \"\"\"\n",
        "  return avg, std\n",
        "  \n",
        "\n",
        "#1- Randomly split data into 5 folds. That means 30 records will be in each fold.\n",
        "\n",
        "\n",
        "def cross_validation_split(x, k_folds):\n",
        "  #This function splits a dataset into k folds\n",
        "  k_folds = int(k_folds)\n",
        "  length = int(len(x)/k_folds)\n",
        "  x = random.choices(x, k=length)\n",
        "  return x\n",
        "\n",
        "\n",
        "#3- Calculate accuracy percentage of the predictions. Remember to average the results of k folds\n",
        "\n",
        "def accurate(y_pred, y):\n",
        "    acc = 0\n",
        "    differences = []\n",
        "    n_sample = len(y)\n",
        "    threshold = 0.2\n",
        "    for i in range (n_sample):\n",
        "      diff = y_pred[i] - y[i]\n",
        "      if (diff < threshold):\n",
        "        acc += 1\n",
        "        #print(\"acc is: \", acc)\n",
        "      differences.append(diff)  \n",
        "    perc = round(acc / n_sample * 100, 2)\n",
        "    print(\"The accuracy is: \", perc)\n",
        "    #print(differences)\n",
        "    return perc\n",
        "\n",
        "#4- Calculate the Gaussian probability distribution function for the given x.\n",
        "\n",
        "def gaussian_probability(x , mean , sd):\n",
        "    prob_density = (np.pi*sd) * np.exp(-0.5*((x-mean)/sd)**2)\n",
        "    return prob_density\n",
        "\n",
        "#2- Predict the class label for a given x.\n",
        "#5- Calculate the probabilities of predicting each class for a given x using naive bayes algorithm. Be aware that we have multiple input variables. \n",
        "#And you may want to use gaussian_probability function here.\n",
        "\n",
        "def predict(x_5, input):\n",
        "  global data\n",
        "  pr_0 = []\n",
        "  pr_1 = [] \n",
        "  x_0 = []\n",
        "  x_1 = []\n",
        "  for i in range(len(x_5)):\n",
        "    if (x_5[i][2] == 0):\n",
        "      x_0.append(x_5[i])\n",
        "    elif (x_5[i][2] == 1):\n",
        "      x_1.append(x_5[i])\n",
        "\n",
        "  avg_0, std_0 = stats(x_0, 0)\n",
        "  avg_1, std_1 = stats(x_1, 1)\n",
        "\n",
        "  data.avg = []\n",
        "  data.std = []\n",
        "  data.avg.append(avg_0)\n",
        "  data.avg.append(avg_1)\n",
        "  data.std.append(std_0)\n",
        "  data.std.append(std_1)\n",
        "\n",
        "  final_0 = (gaussian_probability(input[0], data.avg[0], data.std[0]))*(gaussian_probability(input[1], data.avg[0], data.std[0]))\n",
        "  final_1 = (gaussian_probability(input[0], data.avg[1], data.std[1]))*(gaussian_probability(input[1], data.avg[1], data.std[1]))  \n",
        "  \n",
        "  if (final_0 > final_1):\n",
        "    classification = 0\n",
        "  elif (final_0 < final_1):\n",
        "    classification = 1\n",
        "  return classification     \n",
        "\n",
        "\n",
        "#def class_probability():\n",
        "\n",
        "def plotting(x, y):\n",
        "  plt.xlabel(\"Feature number 1\")\n",
        "  plt.xlabel(\"Feature number 2\")\n",
        "  plt.scatter(x, y, color = \"red\")\n",
        "  plt.scatter(x, y, color = \"blue\")\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "def draw_model(y_pred, y):\n",
        "  plt.xlabel(\"Iteration number\")\n",
        "  plt.ylabel(\"Error\")\n",
        "  plt.title(\"Sampling experiment\")\n",
        "  x = np.arange(0, len(y))\n",
        "  plt.scatter(x, y, color = 'red')\n",
        "  plt.scatter(x, y_pred, color = \"blue\")\n",
        "  plt.show()\n",
        "\n",
        "def true_index(y):\n",
        "  indexes_true = []\n",
        "  for i in range(len(y)):\n",
        "    if(y[i] == 1):\n",
        "      indexes_true.append(i)\n",
        "  return indexes_true\n",
        "\n",
        "y_pred = []\n",
        "y = []\n",
        "\n",
        "epochs = 1000\n",
        "for i in range(epochs):\n",
        "  x_5 = cross_validation_split(x, 5)\n",
        "  index = random.randint(0, 19)\n",
        "  y_pred.append(predict(x_5, x[index]))\n",
        "  y.append(x_5[index][2])\n",
        "\n",
        "\n",
        "#*Print out your accuracy result. Is it good? If not, analyze the reason in short. *\n",
        "#draw_model(y_pred, y)\n",
        "#print(y)\n",
        "#print(y_pred)\n",
        "\n",
        "accuracy = accurate(y_pred, y)\n",
        "if (accuracy > 85.0):\n",
        "  print(\"The accuracy is really good\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy is:  92.4\n",
            "The accuracy is really good\n"
          ]
        }
      ]
    }
  ]
}